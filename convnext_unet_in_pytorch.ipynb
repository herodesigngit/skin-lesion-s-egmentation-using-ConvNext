{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0334799e"
      },
      "outputs": [],
      "source": [
        "from ctypes import resize\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import cv2\n",
        "import os\n",
        "import math\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import h5py\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from functools import partial\n",
        "from typing import Any, Callable, List, Optional, Sequence\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import functional as F\n",
        "\n",
        "tr = torch"
      ],
      "id": "0334799e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czbzFaCUPfVX",
        "outputId": "87c00616-d9d6-4687-df22-0d2eedf9e5c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "czbzFaCUPfVX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hHd_12wP06H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#trainimages=np.load('/content/drive/My Drive/trainimages-fold1.npy')\n",
        "\n",
        "#trainimages=np.load('/content/drive/My Drive/train_seg_images-fold1.npy')\n",
        "\n",
        "trainlabels=np.load('/content/drive/My Drive/trainlabels-fold1.npy')\n",
        "\n",
        "#trainlabels=np.load('/content/drive/My Drive/train_aug_labels-fold1.npy')\n",
        "\n",
        "#testimages=np.load('/content/drive/My Drive/testimages-fold1.npy')\n",
        "\n",
        "testlabels=np.load('/content/drive/My Drive/testlabels-fold1.npy')\n",
        "\n",
        "print(trainimages.shape)\n",
        "print(trainlabels.shape)\n",
        "print(testimages.shape)\n",
        "print(testlabels.shape)"
      ],
      "id": "4hHd_12wP06H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfzCPXT9N5WM"
      },
      "outputs": [],
      "source": [
        "np.savetxt('/content/drive/My Drive/train_gwo.csv', x_train, delimiter=',', fmt='%f')\n",
        "np.savetxt('/content/drive/My Drive/test_gwo.csv', x_valid, delimiter=',', fmt='%f')"
      ],
      "id": "VfzCPXT9N5WM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuYxhmjdhXRx"
      },
      "outputs": [],
      "source": [
        "x_train = pd.read_csv(\"/content/drive/My Drive/train_gwo.csv\", header=None)\n",
        "\n",
        "x_train.shape\n",
        "\n",
        "x_train = np.array(x_train,dtype = 'float16')\n",
        "\n",
        "\n",
        "x_valid = pd.read_csv(\"/content/drive/My Drive/test_gwo.csv\", header=None)\n",
        "\n",
        "x_valid.shape\n",
        "\n",
        "x_valid =np.array(x_valid,dtype = 'float16')"
      ],
      "id": "xuYxhmjdhXRx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWB4edbvLnqo",
        "outputId": "0b39d1b3-9e6f-47ae-ae36-56f43fa38c2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from tqdm import tqdm\n",
        "\n",
        "import albumentations as A\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "IMG_PATH = '/content/drive/MyDrive/segmentation/ISIC2018_Task1-2_Training_Input'\n",
        "#IMG_PATH = '/content/drive/MyDrive/segmentation/ISIC2018_Task1-2_Training_Input_Contrast_Enhanced'\n",
        "MASK_PATH = '/content/drive/MyDrive/segmentation/ISIC2018_Task1_Training_GroundTruth'\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE= 4\n",
        "SIZE = 256\n",
        "LEARNING_RATE = 0.0003\n",
        "NUM_EPOCHS = 20\n",
        "NUM_WORKERS = 2\n",
        "PIN_MEMORY = True\n",
        "\n",
        "class ISICDataset(Dataset):\n",
        "    def __init__(self, images_path, masks_path, size, transform=None):\n",
        "        self.images_path = images_path\n",
        "        self.masks_path = masks_path\n",
        "        self.transform = transform\n",
        "        #print(\"images_path---------------------------> \",images_path)\n",
        "        self.ids = [image_file[:-4] for image_file in os.listdir(images_path) if image_file.endswith('.jpg')]\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.images_path, self.ids[idx] + '.jpg')\n",
        "\n",
        "        mask_path = os.path.join(self.masks_path, self.ids[idx] + '_segmentation.png')\n",
        "\n",
        "        # Load image and mask\n",
        "        img = cv2.imread(os.path.join(self.images_path, self.ids[idx] + '.jpg'), cv2.IMREAD_COLOR)\n",
        "        mask = cv2.imread(os.path.join(self.masks_path, self.ids[idx] + '_segmentation.png'), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "         # Convert to RGB, And convert mask to binary\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        ret, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmentations = self.transform(image=img, mask=mask)\n",
        "            img = augmentations['image']\n",
        "            mask = augmentations['mask']\n",
        "\n",
        "        # Convert numpy arrays to PyTorch tensors\n",
        "        img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.\n",
        "        mask = torch.from_numpy(mask).unsqueeze(0).float()\n",
        "        #mask = torch.from_numpy(mask).float()\n",
        "        mask[mask == 255.0] = 1.0\n",
        "\n",
        "        return img, mask"
      ],
      "id": "pWB4edbvLnqo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bI5hqP0kLl_M"
      },
      "outputs": [],
      "source": [
        "transform = A.Compose(\n",
        "[\n",
        "    A.Resize(height=SIZE, width=SIZE),\n",
        "    A.Rotate(limit=35, p=1.0),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.1)\n",
        "])\n",
        "\n",
        "\n",
        "dataset = ISICDataset(images_path=IMG_PATH,\n",
        "                            masks_path=MASK_PATH, size=SIZE, transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_dataset = Subset(dataset, range(train_size))\n",
        "test_dataset = Subset(dataset, range(train_size, len(dataset)))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ],
      "id": "bI5hqP0kLl_M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "748609cf",
        "outputId": "c29db7fc-fe52-42fe-921a-06a89bed6dea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "519"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_loader)"
      ],
      "id": "748609cf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8991072c"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class Conv2dSamePadding(nn.Conv2d):\n",
        "    \"\"\"2D Convolutions with same padding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True, name=None):\n",
        "        super().__init__(in_channels, out_channels, kernel_size, stride, padding=0, dilation=dilation, groups=groups,\n",
        "                         bias=bias)\n",
        "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n",
        "        self.name = name\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_h, input_w = x.size()[2:]\n",
        "        kernel_h, kernel_w = self.weight.size()[2:]\n",
        "        stride_h, stride_w = self.stride\n",
        "        output_h, output_w = math.ceil(input_h / stride_h), math.ceil(input_w / stride_w)\n",
        "        pad_h = max((output_h - 1) * self.stride[0] + (kernel_h - 1) * self.dilation[0] + 1 - input_h, 0)\n",
        "        pad_w = max((output_w - 1) * self.stride[1] + (kernel_w - 1) * self.dilation[1] + 1 - input_w, 0)\n",
        "        if pad_h > 0 or pad_w > 0:\n",
        "            x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2])\n",
        "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "\n",
        "\n",
        "class BatchNorm2d(nn.BatchNorm2d):\n",
        "    def __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, name=None):\n",
        "        super().__init__(num_features, eps=eps, momentum=momentum, affine=affine,\n",
        "                         track_running_stats=track_running_stats)\n",
        "        self.name = name\n",
        "\n",
        "\n",
        "def drop_connect(inputs, drop_connect_rate, training):\n",
        "    if not training:\n",
        "        return inputs\n",
        "    batch_size = inputs.shape[0]\n",
        "    keep_prob = 1.0 - drop_connect_rate\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n",
        "    binary_tensor = torch.floor(random_tensor)\n",
        "    output = inputs / keep_prob * binary_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class MBConvBlock(nn.Module):\n",
        "    \"\"\"Mobile Inverted Residual Bottleneck Block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, block_args, global_params, idx):\n",
        "        super().__init__()\n",
        "\n",
        "        block_name = 'blocks_' + str(idx) + '_'\n",
        "\n",
        "        self.block_args = block_args\n",
        "        self.batch_norm_momentum = 1 - global_params.batch_norm_momentum\n",
        "        self.batch_norm_epsilon = global_params.batch_norm_epsilon\n",
        "        self.has_se = (self.block_args.se_ratio is not None) and (0 < self.block_args.se_ratio <= 1)\n",
        "        self.id_skip = block_args.id_skip\n",
        "\n",
        "        self.swish = Swish(block_name + '_swish')\n",
        "\n",
        "        # Expansion phase\n",
        "        in_channels = self.block_args.input_filters\n",
        "        out_channels = self.block_args.input_filters * self.block_args.expand_ratio\n",
        "        if self.block_args.expand_ratio != 1:\n",
        "            self._expand_conv = Conv2dSamePadding(in_channels=in_channels,\n",
        "                                                  out_channels=out_channels,\n",
        "                                                  kernel_size=1,\n",
        "                                                  bias=False,\n",
        "                                                  name=block_name + 'expansion_conv')\n",
        "            self._bn0 = BatchNorm2d(num_features=out_channels,\n",
        "                                    momentum=self.batch_norm_momentum,\n",
        "                                    eps=self.batch_norm_epsilon,\n",
        "                                    name=block_name + 'expansion_batch_norm')\n",
        "\n",
        "        # Depth-wise convolution phase\n",
        "        kernel_size = self.block_args.kernel_size\n",
        "        strides = self.block_args.strides\n",
        "        self._depthwise_conv = Conv2dSamePadding(in_channels=out_channels,\n",
        "                                                 out_channels=out_channels,\n",
        "                                                 groups=out_channels,\n",
        "                                                 kernel_size=kernel_size,\n",
        "                                                 stride=strides,\n",
        "                                                 bias=False,\n",
        "                                                 name=block_name + 'depthwise_conv')\n",
        "        self._bn1 = BatchNorm2d(num_features=out_channels,\n",
        "                                momentum=self.batch_norm_momentum,\n",
        "                                eps=self.batch_norm_epsilon,\n",
        "                                name=block_name + 'depthwise_batch_norm')\n",
        "\n",
        "        # Squeeze and Excitation layer\n",
        "        if self.has_se:\n",
        "            num_squeezed_channels = max(1, int(self.block_args.input_filters * self.block_args.se_ratio))\n",
        "            self._se_reduce = Conv2dSamePadding(in_channels=out_channels,\n",
        "                                                out_channels=num_squeezed_channels,\n",
        "                                                kernel_size=1,\n",
        "                                                name=block_name + 'se_reduce')\n",
        "            self._se_expand = Conv2dSamePadding(in_channels=num_squeezed_channels,\n",
        "                                                out_channels=out_channels,\n",
        "                                                kernel_size=1,\n",
        "                                                name=block_name + 'se_expand')\n",
        "\n",
        "        # Output phase\n",
        "        final_output_channels = self.block_args.output_filters\n",
        "        self._project_conv = Conv2dSamePadding(in_channels=out_channels,\n",
        "                                               out_channels=final_output_channels,\n",
        "                                               kernel_size=1,\n",
        "                                               bias=False,\n",
        "                                               name=block_name + 'output_conv')\n",
        "        self._bn2 = BatchNorm2d(num_features=final_output_channels,\n",
        "                                momentum=self.batch_norm_momentum,\n",
        "                                eps=self.batch_norm_epsilon,\n",
        "                                name=block_name + 'output_batch_norm')\n",
        "\n",
        "    def forward(self, x, drop_connect_rate=None):\n",
        "        identity = x\n",
        "        # Expansion and depth-wise convolution\n",
        "        if self.block_args.expand_ratio != 1:\n",
        "            x = self._expand_conv(x)\n",
        "            x = self._bn0(x)\n",
        "            x = self.swish(x)\n",
        "\n",
        "        x = self._depthwise_conv(x)\n",
        "        x = self._bn1(x)\n",
        "        x = self.swish(x)\n",
        "\n",
        "        # Squeeze and Excitation\n",
        "        if self.has_se:\n",
        "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
        "            x_squeezed = self._se_expand(self.swish(self._se_reduce(x_squeezed)))\n",
        "            x = torch.sigmoid(x_squeezed) * x\n",
        "\n",
        "        x = self._bn2(self._project_conv(x))\n",
        "\n",
        "        # Skip connection and drop connect\n",
        "        input_filters, output_filters = self.block_args.input_filters, self.block_args.output_filters\n",
        "        if self.id_skip and self.block_args.strides == 1 and input_filters == output_filters:\n",
        "            if drop_connect_rate:\n",
        "                x = drop_connect(x, drop_connect_rate=drop_connect_rate, training=self.training)\n",
        "            x = x + identity\n",
        "        return x\n",
        "\n",
        "\n",
        "def double_conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "\n",
        "def up_conv(in_channels, out_channels):\n",
        "    return nn.ConvTranspose2d(\n",
        "        in_channels, out_channels, kernel_size=2, stride=2\n",
        "    )\n",
        "\n",
        "\n",
        "def custom_head(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(in_channels, 512),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(512, out_channels)\n",
        "    )"
      ],
      "id": "8991072c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acc59d82"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from typing import Any, Callable, List, Optional, Sequence\n",
        "from typing import Callable, List, Optional, Sequence, Tuple, Union\n",
        "import collections\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import functional as F\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from itertools import repeat\n",
        "\n",
        "\n",
        "def _make_ntuple(x: Any, n: int) -> Tuple[Any, ...]:\n",
        "    \"\"\"\n",
        "    Make n-tuple from input x. If x is an iterable, then we just convert it to tuple.\n",
        "    Otherwise we will make a tuple of length n, all with value of x.\n",
        "    reference: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/utils.py#L8\n",
        "    Args:\n",
        "        x (Any): input value\n",
        "        n (int): length of the resulting tuple\n",
        "    \"\"\"\n",
        "    if isinstance(x, collections.abc.Iterable):\n",
        "        return tuple(x)\n",
        "    return tuple(repeat(x, n))\n",
        "\n",
        "def stochastic_depth(input: Tensor, p: float, mode: str, training: bool = True) -> Tensor:\n",
        "    \"\"\"\n",
        "    Implements the Stochastic Depth from `\"Deep Networks with Stochastic Depth\"\n",
        "    <https://arxiv.org/abs/1603.09382>`_ used for randomly dropping residual\n",
        "    branches of residual architectures.\n",
        "    Args:\n",
        "        input (Tensor[N, ...]): The input tensor or arbitrary dimensions with the first one\n",
        "                    being its batch i.e. a batch with ``N`` rows.\n",
        "        p (float): probability of the input to be zeroed.\n",
        "        mode (str): ``\"batch\"`` or ``\"row\"``.\n",
        "                    ``\"batch\"`` randomly zeroes the entire input, ``\"row\"`` zeroes\n",
        "                    randomly selected rows from the batch.\n",
        "        training: apply stochastic depth if is ``True``. Default: ``True``\n",
        "    Returns:\n",
        "        Tensor[N, ...]: The randomly zeroed tensor.\n",
        "    \"\"\"\n",
        "    # if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n",
        "    #     _log_api_usage_once(stochastic_depth)\n",
        "    if p < 0.0 or p > 1.0:\n",
        "        raise ValueError(f\"drop probability has to be between 0 and 1, but got {p}\")\n",
        "    if mode not in [\"batch\", \"row\"]:\n",
        "        raise ValueError(f\"mode has to be either 'batch' or 'row', but got {mode}\")\n",
        "    if not training or p == 0.0:\n",
        "        return input\n",
        "\n",
        "    survival_rate = 1.0 - p\n",
        "    if mode == \"row\":\n",
        "        size = [input.shape[0]] + [1] * (input.ndim - 1)\n",
        "    else:\n",
        "        size = [1] * input.ndim\n",
        "    noise = torch.empty(size, dtype=input.dtype, device=input.device)\n",
        "    noise = noise.bernoulli_(survival_rate)\n",
        "    if survival_rate > 0.0:\n",
        "        noise.div_(survival_rate)\n",
        "    return input * noise\n",
        "\n",
        "class StochasticDepth(nn.Module):\n",
        "    \"\"\"\n",
        "    See :func:`stochastic_depth`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p: float, mode: str) -> None:\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.mode = mode\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        return stochastic_depth(input, self.p, self.mode, self.training)\n",
        "\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        s = f\"{self.__class__.__name__}(p={self.p}, mode={self.mode})\"\n",
        "        return s\n",
        "\n",
        "class Permute(torch.nn.Module):\n",
        "    \"\"\"This module returns a view of the tensor input with its dimensions permuted.\n",
        "    Args:\n",
        "        dims (List[int]): The desired ordering of dimensions\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dims: List[int]):\n",
        "        super().__init__()\n",
        "        self.dims = dims\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return torch.permute(x, self.dims)\n",
        "\n",
        "class FrozenBatchNorm2d(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    BatchNorm2d where the batch statistics and the affine parameters are fixed\n",
        "    Args:\n",
        "        num_features (int): Number of features ``C`` from an expected input of size ``(N, C, H, W)``\n",
        "        eps (float): a value added to the denominator for numerical stability. Default: 1e-5\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_features: int,\n",
        "        eps: float = 1e-5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.register_buffer(\"weight\", torch.ones(num_features))\n",
        "        self.register_buffer(\"bias\", torch.zeros(num_features))\n",
        "        self.register_buffer(\"running_mean\", torch.zeros(num_features))\n",
        "        self.register_buffer(\"running_var\", torch.ones(num_features))\n",
        "\n",
        "    def _load_from_state_dict(\n",
        "        self,\n",
        "        state_dict: dict,\n",
        "        prefix: str,\n",
        "        local_metadata: dict,\n",
        "        strict: bool,\n",
        "        missing_keys: List[str],\n",
        "        unexpected_keys: List[str],\n",
        "        error_msgs: List[str],\n",
        "    ):\n",
        "        num_batches_tracked_key = prefix + \"num_batches_tracked\"\n",
        "        if num_batches_tracked_key in state_dict:\n",
        "            del state_dict[num_batches_tracked_key]\n",
        "\n",
        "        super()._load_from_state_dict(\n",
        "            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # move reshapes to the beginning\n",
        "        # to make it fuser-friendly\n",
        "        w = self.weight.reshape(1, -1, 1, 1)\n",
        "        b = self.bias.reshape(1, -1, 1, 1)\n",
        "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
        "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
        "        scale = w * (rv + self.eps).rsqrt()\n",
        "        bias = b - rm * scale\n",
        "        return x * scale + bias\n",
        "\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"{self.__class__.__name__}({self.weight.shape[0]}, eps={self.eps})\"\n",
        "\n",
        "\n",
        "\n",
        "class ConvNormActivation(torch.nn.Sequential):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: Union[int, Tuple[int, ...]] = 3,\n",
        "        stride: Union[int, Tuple[int, ...]] = 1,\n",
        "        padding: Optional[Union[int, Tuple[int, ...], str]] = None,\n",
        "        groups: int = 1,\n",
        "        norm_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.BatchNorm2d,\n",
        "        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n",
        "        dilation: Union[int, Tuple[int, ...]] = 1,\n",
        "        inplace: Optional[bool] = True,\n",
        "        bias: Optional[bool] = None,\n",
        "        conv_layer: Callable[..., torch.nn.Module] = torch.nn.Conv2d,\n",
        "    ) -> None:\n",
        "\n",
        "        if padding is None:\n",
        "            if isinstance(kernel_size, int) and isinstance(dilation, int):\n",
        "                padding = (kernel_size - 1) // 2 * dilation\n",
        "            else:\n",
        "                _conv_dim = len(kernel_size) if isinstance(kernel_size, Sequence) else len(dilation)\n",
        "                kernel_size = _make_ntuple(kernel_size, _conv_dim)\n",
        "                dilation = _make_ntuple(dilation, _conv_dim)\n",
        "                padding = tuple((kernel_size[i] - 1) // 2 * dilation[i] for i in range(_conv_dim))\n",
        "        if bias is None:\n",
        "            bias = norm_layer is None\n",
        "\n",
        "        layers = [\n",
        "            conv_layer(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                dilation=dilation,\n",
        "                groups=groups,\n",
        "                bias=bias,\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        if norm_layer is not None:\n",
        "            layers.append(norm_layer(out_channels))\n",
        "\n",
        "        if activation_layer is not None:\n",
        "            params = {} if inplace is None else {\"inplace\": inplace}\n",
        "            layers.append(activation_layer(**params))\n",
        "        super().__init__(*layers)\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        if self.__class__ == ConvNormActivation:\n",
        "            warnings.warn(\n",
        "                \"Don't use ConvNormActivation directly, please use Conv2dNormActivation and Conv3dNormActivation instead.\"\n",
        "            )\n",
        "\n",
        "\n",
        "class Conv2dNormActivation(ConvNormActivation):\n",
        "    \"\"\"\n",
        "    Configurable block used for Convolution2d-Normalization-Activation blocks.\n",
        "    Args:\n",
        "        in_channels (int): Number of channels in the input image\n",
        "        out_channels (int): Number of channels produced by the Convolution-Normalization-Activation block\n",
        "        kernel_size: (int, optional): Size of the convolving kernel. Default: 3\n",
        "        stride (int, optional): Stride of the convolution. Default: 1\n",
        "        padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: None, in which case it will calculated as ``padding = (kernel_size - 1) // 2 * dilation``\n",
        "        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
        "        norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer wont be used. Default: ``torch.nn.BatchNorm2d``\n",
        "        activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer wont be used. Default: ``torch.nn.ReLU``\n",
        "        dilation (int): Spacing between kernel elements. Default: 1\n",
        "        inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True``\n",
        "        bias (bool, optional): Whether to use bias in the convolution layer. By default, biases are included if ``norm_layer is None``.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        kernel_size: Union[int, Tuple[int, int]] = 3,\n",
        "        stride: Union[int, Tuple[int, int]] = 1,\n",
        "        padding: Optional[Union[int, Tuple[int, int], str]] = None,\n",
        "        groups: int = 1,\n",
        "        norm_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.BatchNorm2d,\n",
        "        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n",
        "        dilation: Union[int, Tuple[int, int]] = 1,\n",
        "        inplace: Optional[bool] = True,\n",
        "        bias: Optional[bool] = None,\n",
        "    ) -> None:\n",
        "\n",
        "        super().__init__(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            padding,\n",
        "            groups,\n",
        "            norm_layer,\n",
        "            activation_layer,\n",
        "            dilation,\n",
        "            inplace,\n",
        "            bias,\n",
        "            torch.nn.Conv2d,\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "class LayerNorm2d(nn.LayerNorm):\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CNBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        layer_scale: float,\n",
        "        stochastic_depth_prob: float,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = partial(nn.LayerNorm, eps=1e-6)\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim, bias=True),\n",
        "            Permute([0, 2, 3, 1]),\n",
        "            norm_layer(dim),\n",
        "            nn.Linear(in_features=dim, out_features=4 * dim, bias=True),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(in_features=4 * dim, out_features=dim, bias=True),\n",
        "            Permute([0, 3, 1, 2]),\n",
        "        )\n",
        "        self.layer_scale = nn.Parameter(torch.ones(dim, 1, 1) * layer_scale)\n",
        "        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, \"row\")\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        result = self.layer_scale * self.block(input)\n",
        "        result = self.stochastic_depth(result)\n",
        "        result += input\n",
        "        return result\n",
        "\n",
        "\n",
        "class CNBlockConfig:\n",
        "    # Stores information listed at Section 3 of the ConvNeXt paper\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_channels: int,\n",
        "        out_channels: Optional[int],\n",
        "        num_layers: int,\n",
        "    ) -> None:\n",
        "        self.input_channels = input_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        s = self.__class__.__name__ + \"(\"\n",
        "        s += \"input_channels={input_channels}\"\n",
        "        s += \", out_channels={out_channels}\"\n",
        "        s += \", num_layers={num_layers}\"\n",
        "        s += \")\"\n",
        "        return s.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ConvNeXt(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block_setting: List[CNBlockConfig],\n",
        "        stochastic_depth_prob: float = 0.0,\n",
        "        layer_scale: float = 1e-6,\n",
        "        num_classes: int = 1000,\n",
        "        block: Optional[Callable[..., nn.Module]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if not block_setting:\n",
        "            raise ValueError(\"The block_setting should not be empty\")\n",
        "        elif not (isinstance(block_setting, Sequence) and all([isinstance(s, CNBlockConfig) for s in block_setting])):\n",
        "            raise TypeError(\"The block_setting should be List[CNBlockConfig]\")\n",
        "\n",
        "        if block is None:\n",
        "            block = CNBlock\n",
        "\n",
        "        if norm_layer is None:\n",
        "            norm_layer = partial(LayerNorm2d, eps=1e-6)\n",
        "\n",
        "        layers: List[nn.Module] = []\n",
        "\n",
        "        # Stem\n",
        "        firstconv_output_channels = block_setting[0].input_channels\n",
        "        # layers.append(\n",
        "        #     Conv2dNormActivation(\n",
        "        #         3,\n",
        "        #         firstconv_output_channels,\n",
        "        #         kernel_size=4,\n",
        "        #         stride=4,\n",
        "        #         padding=0,\n",
        "        #         norm_layer=norm_layer,\n",
        "        #         activation_layer=None,\n",
        "        #         bias=True,\n",
        "        #     )\n",
        "        # )\n",
        "        layers.append(\n",
        "            Conv2dNormActivation(\n",
        "                3,\n",
        "                firstconv_output_channels,\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1,\n",
        "                norm_layer=norm_layer,\n",
        "                activation_layer=None,\n",
        "                bias=True,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        total_stage_blocks = sum(cnf.num_layers for cnf in block_setting)\n",
        "        stage_block_id = 0\n",
        "        for cnf in block_setting:\n",
        "            # Bottlenecks\n",
        "            stage: List[nn.Module] = []\n",
        "            for _ in range(cnf.num_layers):\n",
        "                # adjust stochastic depth probability based on the depth of the stage block\n",
        "                sd_prob = stochastic_depth_prob * stage_block_id / (total_stage_blocks - 1.0)\n",
        "                stage.append(block(cnf.input_channels, layer_scale, sd_prob))\n",
        "                stage_block_id += 1\n",
        "            layers.append(nn.Sequential(*stage))\n",
        "            if cnf.out_channels is not None:\n",
        "                # Downsampling\n",
        "                layers.append(\n",
        "                    nn.Sequential(\n",
        "                        norm_layer(cnf.input_channels),\n",
        "                        nn.Conv2d(cnf.input_channels, cnf.out_channels, kernel_size=2, stride=2),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        lastblock = block_setting[-1]\n",
        "        lastconv_output_channels = (\n",
        "            lastblock.out_channels if lastblock.out_channels is not None else lastblock.input_channels\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            norm_layer(lastconv_output_channels), nn.Flatten(1), nn.Linear(lastconv_output_channels, num_classes)\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "def _convnext(\n",
        "    block_setting: List[CNBlockConfig],\n",
        "    stochastic_depth_prob: float,\n",
        "    weights_url,\n",
        "    progress: bool,\n",
        "    **kwargs: Any,\n",
        "):\n",
        "\n",
        "    model = ConvNeXt(block_setting, stochastic_depth_prob=stochastic_depth_prob, **kwargs)\n",
        "#     if weights_url is not None:\n",
        "#         weights =  load_state_dict_from_url(weights_url, progress=progress)\n",
        "#         model.load_state_dict(weights)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "URL_DICT = {\n",
        "    \"tiny\": \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\",\n",
        "    \"small\": \"https://download.pytorch.org/models/convnext_small-0c510722.pth\",\n",
        "    \"base\": \"https://download.pytorch.org/models/convnext_base-6075fbad.pth\",\n",
        "    \"large\": \"https://download.pytorch.org/models/convnext_large-ea097f82.pth\"\n",
        "}\n",
        "\n",
        "\n",
        "def convnext_tiny(pretrained = False, progress = True, **kwargs):\n",
        "\n",
        "    url = URL_DICT[\"tiny\"]\n",
        "    block_setting = [\n",
        "        CNBlockConfig(96, 192, 3),\n",
        "        CNBlockConfig(192, 384, 3),\n",
        "        CNBlockConfig(384, 768, 9),\n",
        "        CNBlockConfig(768, None, 3),\n",
        "    ]\n",
        "    stochastic_depth_prob = kwargs.pop(\"stochastic_depth_prob\", 0.1)\n",
        "    return _convnext(block_setting, stochastic_depth_prob, url, progress, **kwargs)\n",
        "\n",
        "def convnext_large(pretrained = False, progress = True, **kwargs):\n",
        "\n",
        "    url = URL_DICT[\"large\"]\n",
        "    block_setting = [\n",
        "        CNBlockConfig(192, 384, 3),\n",
        "        CNBlockConfig(384, 768, 3),\n",
        "        CNBlockConfig(768, 1536, 27),\n",
        "        CNBlockConfig(1536, None, 3),\n",
        "    ]\n",
        "    stochastic_depth_prob = kwargs.pop(\"stochastic_depth_prob\", 0.1)\n",
        "    return _convnext(block_setting, stochastic_depth_prob, url, progress, **kwargs)"
      ],
      "id": "acc59d82"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3E-PVAEoBme"
      },
      "outputs": [],
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, f_g, f_l, f_int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.w_g = nn.Sequential(\n",
        "                                nn.Conv2d(f_g, f_int,\n",
        "                                         kernel_size=1, stride=1,\n",
        "                                         padding=0, bias=True),\n",
        "                                nn.BatchNorm2d(f_int)\n",
        "        )\n",
        "\n",
        "        self.w_x = nn.Sequential(\n",
        "                                nn.Conv2d(f_l, f_int,\n",
        "                                         kernel_size=1, stride=1,\n",
        "                                         padding=0, bias=True),\n",
        "                                nn.BatchNorm2d(f_int)\n",
        "        )\n",
        "\n",
        "        self.psi = nn.Sequential(\n",
        "                                nn.Conv2d(f_int, 1,\n",
        "                                         kernel_size=1, stride=1,\n",
        "                                         padding=0,  bias=True),\n",
        "                                nn.BatchNorm2d(1),\n",
        "                                nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        g1 = self.w_g(g)\n",
        "        x1 = self.w_x(x)\n",
        "        psi = self.relu(g1+x1)\n",
        "        psi = self.psi(psi)\n",
        "\n",
        "        return psi*x"
      ],
      "id": "L3E-PVAEoBme"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conv+"
      ],
      "metadata": {
        "id": "e5ipFitADMmr"
      },
      "id": "e5ipFitADMmr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72d51296"
      },
      "outputs": [],
      "source": [
        "def get_blocks_to_be_concat(model, x):\n",
        "    features = {}\n",
        "    def get_features(name):\n",
        "        def hook(model, input, output):\n",
        "            features[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "#     model[1][2].register_forward_hook(get_features('feat1'))\n",
        "#     model[3][2].register_forward_hook(get_features('feat2'))\n",
        "#     model[5][26].register_forward_hook(get_features('feat3'))\n",
        "\n",
        "    model[1][2].register_forward_hook(get_features('feat1'))\n",
        "    model[3][2].register_forward_hook(get_features('feat2'))\n",
        "    model[5][8].register_forward_hook(get_features('feat3'))\n",
        "\n",
        "\n",
        "    # make a forward pass to trigger the hooks\n",
        "    x = model(x)\n",
        "    # print (features[\"feat1\"].size())\n",
        "    # print (features[\"feat2\"].size())\n",
        "    # print (features[\"feat3\"].size())\n",
        "\n",
        "    return x, features\n",
        "\n",
        "\n",
        "class ConvNextUnet(nn.Module):\n",
        "    def __init__(self, encoder,n_channels = 1536, out_channels=1, concat_input=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder.features\n",
        "        print (self.encoder[1][2])\n",
        "        self.concat_input = concat_input\n",
        "\n",
        "        n_channels = 1536\n",
        "        self.size = [1536, 768, 384, 192]\n",
        "\n",
        "        self.up_conv1 = up_conv(n_channels, 768)\n",
        "        self.att3 = AttentionBlock(f_g=768, f_l=768, f_int=384)\n",
        "        self.double_conv1 = double_conv(self.size[0], 768)\n",
        "\n",
        "        self.up_conv2 = up_conv(768, 384)\n",
        "        self.att2 = AttentionBlock(f_g=384, f_l=384, f_int=192)\n",
        "        self.double_conv2 = double_conv(self.size[1], 384)\n",
        "\n",
        "        self.up_conv3 = up_conv(384, 192)\n",
        "        self.att1 = AttentionBlock(f_g=192, f_l=192, f_int=96)\n",
        "        self.double_conv3 = double_conv(self.size[2], 192)\n",
        "\n",
        "        self.up_conv_input = up_conv(192, 96)\n",
        "        self.double_conv_input = double_conv(99, 32)\n",
        "\n",
        "        #self.up_conv_input = up_conv(192, 96)\n",
        "        #self.att0 = AttentionBlock(f_g=96, f_l=96, f_int=16)\n",
        "        #self.double_conv_input = double_conv(99, 32)\n",
        "\n",
        "#         n_channels = 768\n",
        "#         self.size = [768, 384, 192, 96]\n",
        "#         self.up_conv1 = up_conv(n_channels, 384)\n",
        "#         self.double_conv1 = double_conv(self.size[0], 384)\n",
        "\n",
        "#         self.up_conv2 = up_conv(384, 192)\n",
        "#         self.double_conv2 = double_conv(self.size[1], 192)\n",
        "\n",
        "#         self.up_conv3 = up_conv(192, 96)\n",
        "#         self.double_conv3 = double_conv(self.size[2], 96)\n",
        "\n",
        "#         self.up_conv_input = up_conv(96, 32)\n",
        "#         self.double_conv_input = double_conv(35, 16)\n",
        "\n",
        "#         self.final_conv = nn.Conv2d(16, 1, kernel_size=1)\n",
        "        self.final_conv = nn.Conv2d(32, 1, kernel_size=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_ = x\n",
        "        x, blocks = get_blocks_to_be_concat(self.encoder, x)\n",
        "\n",
        "        x = self.up_conv1(x)\n",
        "        x3 = self.att3(g=x, x=blocks[\"feat3\"])\n",
        "        x = torch.cat([x, x3], dim=1)\n",
        "        x = self.double_conv1(x)\n",
        "\n",
        "\n",
        "\n",
        "        x = self.up_conv2(x)\n",
        "        x2 = self.att2(g=x, x=blocks[\"feat2\"])\n",
        "        x = torch.cat([x, x2], dim=1)\n",
        "        x = self.double_conv2(x)\n",
        "\n",
        "\n",
        "\n",
        "        x = self.up_conv3(x)\n",
        "        x1 = self.att1(g=x, x=blocks[\"feat1\"])\n",
        "        x = torch.cat([x, x1], dim=1)\n",
        "        x = self.double_conv3(x)\n",
        "\n",
        "        #x = self.up_conv_input(x)\n",
        "\n",
        "        #x0 = self.att0(g=x, x=input_)\n",
        "\n",
        "        #x = torch.cat([x, x0], dim=1)\n",
        "\n",
        "        x = self.up_conv_input(x)\n",
        "        x = torch.cat([x, input_], dim=1)\n",
        "        x = self.double_conv_input(x)\n",
        "\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def get_encoder():\n",
        "    model = convnext_large(pretrained = False)\n",
        "    return model\n",
        "\n",
        "def create_convnext_unet():\n",
        "    encoder = get_encoder()\n",
        "    model = ConvNextUnet(encoder)\n",
        "    return model"
      ],
      "id": "72d51296"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "957691ea",
        "outputId": "3e75ee98-3fb3-4cd2-cac0-8343a2168bad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up seeds!\n",
            "Setting up cudnn!\n"
          ]
        }
      ],
      "source": [
        "def create_efficient_net():\n",
        "    # unet = get_efficientunet_b7(out_channels=1, concat_input=True, pretrained=True).cuda()\n",
        "    unet = create_convnext_unet()\n",
        "    return unet\n",
        "def create_optimizer(model):\n",
        "\n",
        "    base_lr = 1e-4\n",
        "    no_weight_decay_on_bn = False\n",
        "    params_to_update = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            params_to_update.append(param)\n",
        "    optimizer = torch.optim.AdamW(params_to_update,lr = base_lr)\n",
        "    return optimizer\n",
        "\n",
        "class CreateLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CreateLoss, self).__init__()\n",
        "        self.loss_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def forward(self, res_target, masks_target):\n",
        "        res_target = res_target.squeeze(1)\n",
        "        loss_target = self.loss_criterion(res_target, masks_target)\n",
        "        return loss_target\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"\n",
        "    Class for calculating average\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, num):\n",
        "        self.val = val\n",
        "        self.sum += val * num\n",
        "        self.count += num\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def set_seeds():\n",
        "    print (\"Setting up seeds!\")\n",
        "    random.seed(0)\n",
        "    np.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    torch.cuda.manual_seed(0)\n",
        "\n",
        "def setup_cudnn(cudnn_benchmark, cudnn_deterministic):\n",
        "    print (\"Setting up cudnn!\")\n",
        "    torch.backends.cudnn.benchmark = cudnn_benchmark\n",
        "    torch.backends.cudnn.deterministic = cudnn_deterministic\n",
        "\n",
        "def MakeDir(path):\n",
        "    if (not os.path.exists(path)):\n",
        "        os.mkdir(path)\n",
        "use_cuda = True\n",
        "cuda_device_id = 0\n",
        "cudnn_benchmark = False\n",
        "cudnn_deterministic = False\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "# working_dir = \"/kaggle/working/\"\n",
        "# print(f\"The current working directory is {working_dir}\")\n",
        "# MakeDir(working_dir + \"/checkpoint_save/\")\n",
        "if (use_cuda):\n",
        "    torch.cuda.set_device(cuda_device_id)\n",
        "set_seeds()\n",
        "setup_cudnn(cudnn_benchmark, cudnn_deterministic)"
      ],
      "id": "957691ea"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9zF9zriY4DR"
      },
      "outputs": [],
      "source": [
        "use_cuda = True\n",
        "device_id = 0\n",
        "# model_path = \"./checkpoint_save/checkpoint_0020.pth\"\n",
        "model_path = \"/kaggle/input/hubmap-convnext-checkpoint/checkpoint_0030.pth\"\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "if (use_cuda):\n",
        "    torch.cuda.set_device(device_id)\n"
      ],
      "id": "r9zF9zriY4DR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilg1gL7IBF_x"
      },
      "source": [
        "filepath"
      ],
      "id": "Ilg1gL7IBF_x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-XEk06dOSyf"
      },
      "outputs": [],
      "source": [
        "def check_accuracy(loader, model, device=\"cuda\"):\n",
        "    num_correct = 0\n",
        "    dice_score = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            preds = torch.sigmoid(model(x))\n",
        "            preds = (preds > 0.5).float()\n",
        "            num_correct += (preds == y).sum()\n",
        "            dice_score += (2 * (preds * y).sum()) / (preds + y).sum() + 1e-8\n",
        "\n",
        "    print(f\"Dice score: {dice_score/len(loader)}\")\n",
        "    model.train()"
      ],
      "id": "8-XEk06dOSyf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5gU5TviOSwh"
      },
      "outputs": [],
      "source": [
        "check_accuracy(test_loader, model, device)"
      ],
      "id": "x5gU5TviOSwh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPOTp2bO5pa-"
      },
      "source": [
        "Load model"
      ],
      "id": "DPOTp2bO5pa-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENSrj_otlH42"
      },
      "outputs": [],
      "source": [
        "use_cuda = True\n",
        "device_id = 0\n",
        "# model_path = \"./checkpoint_save/checkpoint_0020.pth\"\n",
        "model_path = \"/kaggle/input/hubmap-convnext-checkpoint/checkpoint_0030.pth\"\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "if (use_cuda):\n",
        "    torch.cuda.set_device(device_id)\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "if (use_cuda):\n",
        "    torch.cuda.set_device(device_id)\n",
        "\n",
        "\n",
        "model = create_efficient_net()\n",
        "#model.load_state_dict(torch.load(\"/content/drive/MyDrive/convnextmodel_att_5.pt\"))\n",
        "model.eval()\n",
        "model = model.to(device)\n"
      ],
      "id": "ENSrj_otlH42"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYROo8HSVUh8"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from skimage import data, io\n",
        "\n",
        "train_seg_images=[]\n",
        "\n",
        "\n",
        "test_seg_images=[]\n",
        "\n",
        "\n",
        "\n",
        "for i in range(8012):\n",
        "\n",
        "  print(i)\n",
        "\n",
        "  #io.imshow(trainimages[i].astype('uint8'))\n",
        "  #io.show()\n",
        "\n",
        "  img = torch.from_numpy(trainimages[i]).permute(2, 0, 1).unsqueeze(0).float()/ 255.\n",
        "\n",
        "  img = img.to(device)\n",
        "\n",
        "  res = model(img.to(device))\n",
        "\n",
        "  mask =res\n",
        "\n",
        "#mask = mask.squeeze(0).permute(1, 2, 0).detach().cpu().numpy()\n",
        "\n",
        "  mask = mask.squeeze().detach().cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "  _, mask = cv2.threshold(mask, thresh=0.5, maxval=1, type=cv2.THRESH_BINARY)\n",
        "\n",
        "\n",
        "  mask = np.expand_dims(mask, axis=2)\n",
        "\n",
        "  mask = np.repeat(mask, 3, axis=2)\n",
        "\n",
        "  bgcopy =trainimages[i].copy()\n",
        "\n",
        "  mask_ind = (mask == 0)\n",
        "\n",
        "  bgcopy[mask_ind] = mask[mask_ind]\n",
        "\n",
        "  #plt.imshow(bgcopy.astype('uint8'))\n",
        "\n",
        "  #plt.show()\n",
        "\n",
        "  train_seg_images.append(bgcopy.astype('uint8'))\n",
        "\n",
        "\n",
        "train_seg_images = np.array(train_seg_images,dtype = 'float16')\n",
        "\n",
        "np.save(\"/content/drive/My Drive/train_seg_images-fold1.npy\", train_seg_images)\n",
        "\n",
        "train_seg_images.shape"
      ],
      "id": "yYROo8HSVUh8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KMcrI44bZJG"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from skimage import data, io\n",
        "\n",
        "train_seg_images=[]\n",
        "\n",
        "\n",
        "test_seg_images=[]\n",
        "\n",
        "\n",
        "\n",
        "for i in range(2003):\n",
        "\n",
        "  print(i)\n",
        "\n",
        "  #io.imshow(trainimages[i].astype('uint8'))\n",
        "  #io.show()\n",
        "\n",
        "  img = torch.from_numpy(testimages[i]).permute(2, 0, 1).unsqueeze(0).float()/ 255.\n",
        "\n",
        "  img = img.to(device)\n",
        "\n",
        "  res = model(img.to(device))\n",
        "\n",
        "  mask =res\n",
        "\n",
        "#mask = mask.squeeze(0).permute(1, 2, 0).detach().cpu().numpy()\n",
        "\n",
        "  mask = mask.squeeze().detach().cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "  _, mask = cv2.threshold(mask, thresh=0.5, maxval=1, type=cv2.THRESH_BINARY)\n",
        "\n",
        "\n",
        "  mask = np.expand_dims(mask, axis=2)\n",
        "\n",
        "  mask = np.repeat(mask, 3, axis=2)\n",
        "\n",
        "  bgcopy =testimages[i].copy()\n",
        "\n",
        "  mask_ind = (mask == 0)\n",
        "\n",
        "  bgcopy[mask_ind] = mask[mask_ind]\n",
        "\n",
        "  #plt.imshow(bgcopy.astype('uint8'))\n",
        "\n",
        "  #plt.show()\n",
        "\n",
        "  test_seg_images.append(bgcopy.astype('uint8'))\n",
        "\n",
        "\n",
        "test_seg_images = np.array(test_seg_images,dtype = 'float16')\n",
        "\n",
        "np.save(\"/content/drive/My Drive/test_seg_images-fold1.npy\", test_seg_images)\n",
        "\n",
        "test_seg_images.shape"
      ],
      "id": "-KMcrI44bZJG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPpCrAcyjFnp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "trainimages=np.load('/content/drive/My Drive/trainimages-fold1.npy')\n",
        "\n",
        "#trainimages=np.load('/content/drive/My Drive/skin dataset/train_aug_images-fold1.npy')\n",
        "\n",
        "trainlabels=np.load('/content/drive/My Drive/trainlabels-fold1.npy')\n",
        "\n",
        "#trainlabels=np.load('/content/drive/My Drive/skin dataset/train_aug_labels-fold1.npy')\n",
        "\n",
        "testimages=np.load('/content/drive/My Drive/testimages-fold1.npy')\n",
        "testlabels=np.load('/content/drive/My Drive/testlabels-fold1.npy')\n",
        "\n",
        "print(trainimages.shape)\n",
        "print(trainlabels.shape)\n",
        "print(testimages.shape)\n",
        "print(testlabels.shape)"
      ],
      "id": "pPpCrAcyjFnp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d779f723"
      },
      "outputs": [],
      "source": [
        "def create_scheduler(optimizer):\n",
        "    milestones = [40,60,90]\n",
        "    lr_decay = 0.1\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "        optimizer,\n",
        "        milestones=milestones,\n",
        "        gamma=lr_decay)\n",
        "    return scheduler"
      ],
      "id": "d779f723"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1324f7b1"
      },
      "outputs": [],
      "source": [
        "loss_function = CreateLoss()\n",
        "optimizer = create_optimizer(model)\n",
        "scheduler = create_scheduler(optimizer)"
      ],
      "id": "1324f7b1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HOghIaq8H1E"
      },
      "source": [
        "show segmented Images"
      ],
      "id": "_HOghIaq8H1E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQxAXn0cU941"
      },
      "outputs": [],
      "source": [
        "\n",
        "for step, sample in enumerate(test_loader):\n",
        "\n",
        "        #print (\"step Number => \" + str(step))\n",
        "\n",
        "        images, masks = sample\n",
        "\n",
        "        images = images.to(device)\n",
        "\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        res = model(images)\n",
        "        mask =res\n",
        "\n",
        "\n",
        "\n",
        "        image = images[0].permute(1, 2, 0).detach().cpu().numpy()\n",
        "\n",
        "        mask = mask[0].permute(1, 2, 0).detach().cpu().numpy()\n",
        "\n",
        "        actualmask = masks[0].permute(1, 2, 0).detach().cpu().numpy()\n",
        "\n",
        "\n",
        "        mask = cv2.resize(mask, (image.shape[1], image.shape[0]))\n",
        "        mask = np.expand_dims(mask, axis=2)\n",
        "        mask = np.repeat(mask, 3, axis=2)\n",
        "\n",
        "        plt.figure(figsize = (20, 20))\n",
        "\n",
        "        plt.subplot(1,3,1)\n",
        "        plt.imshow(image)\n",
        "        plt.title('Original Image',fontsize = 20)\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1,3,2)\n",
        "        plt.imshow(actualmask)\n",
        "        plt.title('Target Image',fontsize = 20)\n",
        "        plt.axis('off')\n",
        "        plt.subplot(1,3,3)\n",
        "        plt.imshow(mask)\n",
        "        plt.title('Generated Image',fontsize = 20)\n",
        "        plt.axis('off')\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ],
      "id": "OQxAXn0cU941"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5ue7nVvZ-jt"
      },
      "outputs": [],
      "source": [
        "def check_accuracy(loader, model, device=\"cuda\"):\n",
        "    num_correct = 0\n",
        "    dice_score = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            preds = torch.sigmoid(model(x))\n",
        "            preds = (preds > 0.5).float()\n",
        "            num_correct += (preds == y).sum()\n",
        "            dice_score += (2 * (preds * y).sum()) / (preds + y).sum() + 1e-8\n",
        "\n",
        "    print(f\"Dice score: {dice_score/len(loader)}\")\n",
        "    model.train()\n",
        "\n",
        "def accuracyfunc(y_true, y_pred):\n",
        "    '''Calculates the mean accuracy rate across all predictions for binary\n",
        "    classification problems.\n",
        "    '''\n",
        "    return K.mean(K.equal(y_true, K.round(y_pred)))\n",
        "def jaccard_distance(y_true, y_pred, smooth=100):\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
        "    sum_ = K.sum(K.square(y_true), axis = -1) + K.sum(K.square(y_pred), axis=-1)\n",
        "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "    return (1 - jac)\n",
        "\n",
        "def precisionfunc(y_true, y_pred):\n",
        "    '''Calculates the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    '''\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def recallfunc(y_true, y_pred):\n",
        "    '''Calculates the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    '''\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "from keras import backend as K\n",
        "def check_accuracy2(loader, model, device=\"cuda\"):\n",
        "    num_correct = 0\n",
        "    recall = 0\n",
        "    accuracy = 0\n",
        "    precision= 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            #print(\"raw predict\",model(x))\n",
        "            preds = torch.sigmoid(model(x))\n",
        "            #num_correct += (preds == y).sum()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            preds = (preds > 0.5).float()\n",
        "            #print(\"preds\",preds)\n",
        "            #print(\"y\",y)\n",
        "\n",
        "            #num_correct += (preds == y).sum()\n",
        "            prediction = preds.detach().cpu().numpy()\n",
        "            label= y.detach().cpu().numpy()\n",
        "            print(\"result --------------------->\",recallfunc(label,prediction))\n",
        "            recall += recallfunc(label,prediction)\n",
        "            accuracy += accuracyfunc(label,prediction)\n",
        "            precision+=precisionfunc(label,prediction)\n",
        "\n",
        "\n",
        "    print(f\"recall: {recall/len(loader)}\")\n",
        "    print(f\"accuracy: {accuracy/len(loader)}\")\n",
        "    print(f\"precision: {precision/len(loader)}\")\n",
        "    model.train()"
      ],
      "id": "D5ue7nVvZ-jt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcb7170f"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "def train(epoch,model,optimizer,scheduler,loss_function,train_loader):\n",
        "    print (\"Training!\")\n",
        "    use_cuda = True\n",
        "    model.train()\n",
        "    device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "    Tensor = torch.cuda.FloatTensor if (use_cuda and torch.cuda.is_available()) else torch.FloatTensor\n",
        "\n",
        "    acc = 0.\t# Accuracy\n",
        "    SE = 0.\t\t# Sensitivity (Recall)\n",
        "    SP = 0.\t\t# Specificity\n",
        "    PC = 0. \t# Precision\n",
        "    F1 = 0.\t\t# F1 Score\n",
        "    JS = 0.\t\t# Jaccard Similarity\n",
        "    DC = 0.\t\t# Dice Coefficient\n",
        "    length = 0\n",
        "\n",
        "    Train_Loss_meter = AverageMeter()\n",
        "    for step, sample in enumerate(train_loader):\n",
        "\n",
        "        #print (\"step Number => \" + str(step))\n",
        "\n",
        "\n",
        "        images, masks = sample\n",
        "\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        res = model(images)\n",
        "\n",
        "        #.unsqueeze(0)\n",
        "        loss = criterion(res, masks)\n",
        "\n",
        "        print (\"Loss : \" + str(loss))\n",
        "\n",
        "\n",
        "\n",
        "        mask=res\n",
        "\n",
        "\n",
        "        image = images[0].permute(1, 2, 0).detach().cpu().numpy()\n",
        "\n",
        "        mask = mask[0].permute(1, 2, 0).detach().cpu().numpy()\n",
        "\n",
        "        actualmask = masks[0].permute(1, 2, 0).detach().cpu().numpy()\n",
        "\n",
        "\n",
        "        #mask = cv2.resize(mask, (image.shape[1], image.shape[0]))\n",
        "        #mask = np.expand_dims(mask, axis=2)\n",
        "        #mask = np.repeat(mask, 3, axis=2)\n",
        "\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        num = res.size(0)\n",
        "        Train_Loss_meter.update(loss.item(), num)\n",
        "\n",
        "    print (\"Epoch : \" + str(epoch) + \" :: TrainLoss : \" + str(Train_Loss_meter.avg))\n",
        "\n"
      ],
      "id": "bcb7170f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67dc4abf"
      },
      "outputs": [],
      "source": [
        "def validate(epoch,model,loss_function,train_loader):\n",
        "    print (\"Validating!\")\n",
        "    model.eval()\n",
        "    use_cuda = True\n",
        "    device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "    Tensor = torch.cuda.FloatTensor if (use_cuda and torch.cuda.is_available()) else torch.FloatTensor\n",
        "\n",
        "    Train_Loss_meter = AverageMeter()\n",
        "\n",
        "\n",
        "    l = []\n",
        "    for step, sample in enumerate(train_loader):\n",
        "        images, masks = sample\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "        res = model(images)\n",
        "\n",
        "        loss = criterion(res, masks)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        num = res.size(0)\n",
        "        Train_Loss_meter.update(loss.item(), num)\n",
        "\n",
        "    print (\"Epoch : \" + str(epoch) + \" :: TrainLoss : \" + str(Train_Loss_meter.avg))\n",
        "    check_accuracy(test_loader, model, device)\n"
      ],
      "id": "67dc4abf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJrBoruqLXuz",
        "outputId": "b71d94fb-1648-4596-ca52-46cfebc84b71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dice score: 0.8798908591270447\n"
          ]
        }
      ],
      "source": [
        "check_accuracy(test_loader, model, device)"
      ],
      "id": "CJrBoruqLXuz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83eyWmdOITsy"
      },
      "outputs": [],
      "source": [
        "check_accuracy2(test_loader, model, device)"
      ],
      "id": "83eyWmdOITsy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSdpgg5UrOVE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:32'"
      ],
      "id": "TSdpgg5UrOVE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DsFcUwHtnYL"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "id": "5DsFcUwHtnYL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a84c3725"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "# # Starting Training\n",
        "# # -----------------------------------------\n",
        "print (\"Starting Training!\")\n",
        "epochs = 20\n",
        "val_period = 1\n",
        "checkpoint_period = 5\n",
        "print (\"Number of epochs : \" + str(epochs))\n",
        "print (\"Validation Period : \" + str(val_period))\n",
        "print (\"Checkpoint Period : \" + str(checkpoint_period))\n",
        "# # input(\"Press Enter to Start Training Model ?\")\n",
        "for epoch in range(1, epochs + 1):\n",
        "     print (\"Epoch Number => \" + str(epoch))\n",
        "     train(epoch,model,optimizer,scheduler,loss_function,train_loader)\n",
        "\n",
        "     scheduler.step()\n",
        "     if epoch % val_period == 0:\n",
        "          validate(epoch, model, loss_function, test_loader)\n",
        "     if (epoch % checkpoint_period == 0 or epoch == epochs):\n",
        "        checkpoint_config = {'epoch': epoch}\n",
        "        #checkpointer.save(f\"checkpoint_{epoch:04d}\", **checkpoint_config)\n",
        "     filepath=\"/content/drive/MyDrive/convnextmodel_att_\"+ str(epoch)+\".pt\"\n",
        "     torch.save(model.state_dict(), filepath)\n",
        "     print (\"------------------------------------\")\n",
        "\n",
        "#check_accuracy2(test_loader, model, device)\n",
        "check_accuracy(test_loader, model, device)"
      ],
      "id": "a84c3725"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuvRrxppLyuq"
      },
      "outputs": [],
      "source": [
        "filepath=\"/content/drive/MyDrive/convnextmodel.pt\"\n",
        "torch.save(model.state_dict(), filepath)"
      ],
      "id": "FuvRrxppLyuq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXMsqhoaVvBe"
      },
      "source": [
        "precion: **0.9102361798286438**\n",
        "\n",
        "\n",
        "recall: 0.9072438478469849\n",
        "\n",
        "\n",
        "accuracy: 0.9552687406539917\n",
        "\n",
        "Dice score: 0.8995338678359985"
      ],
      "id": "eXMsqhoaVvBe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8l4PRbfMU5M"
      },
      "source": [
        "Dice score: 0.8647632598876953\n",
        "recall: 0.8940997123718262\n",
        "accuracy: 0.9448225498199463\n",
        "precision: 0.8728080987930298\n",
        "\n",
        "\n",
        "> Indented block\n",
        "\n"
      ],
      "id": "w8l4PRbfMU5M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmf3DsRJZ_-z"
      },
      "outputs": [],
      "source": [
        "check_accuracy(test_loader, model, device)"
      ],
      "id": "xmf3DsRJZ_-z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iyylg7W9rtLv"
      },
      "outputs": [],
      "source": [
        "scaler = torch.cuda.amp.GradScaler()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(30):\n",
        "\n",
        "    loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
        "\n",
        "    if epoch % 5 == 0 and epoch != 0:\n",
        "        checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(),\n",
        "                      'epoch': epoch}\n",
        "        torch.save(checkpoint, \"checkpointN\"+str(epoch)+\"_.pth.tar\")\n",
        "\n",
        "\n",
        "    for batch_idx, (images, masks) in loop:\n",
        "        images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
        "\n",
        "        # Gradients to 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "\n",
        "        # Backward\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        loop.set_description(f\"Epoch[{epoch}/{NUM_EPOCHS}]\")\n",
        "        loop.set_postfix(loss = loss.item())\n",
        "        checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "        torch.save(checkpoint, \"checkpoint_last.pth.tar\")\n"
      ],
      "id": "Iyylg7W9rtLv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbcc08e9"
      },
      "outputs": [],
      "source": [
        "use_cuda = True\n",
        "device_id = 0\n",
        "# model_path = \"./checkpoint_save/checkpoint_0020.pth\"\n",
        "model_path = \"/kaggle/input/hubmap-convnext-checkpoint/checkpoint_0030.pth\"\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "if (use_cuda):\n",
        "    torch.cuda.set_device(device_id)"
      ],
      "id": "fbcc08e9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3a648b8"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "if (use_cuda):\n",
        "    torch.cuda.set_device(device_id)\n",
        "\n",
        "model = create_efficient_net()\n",
        "model = model.to(device)\n",
        "# model.load_state_dict(torch.load(model_path)[\"model\"])"
      ],
      "id": "b3a648b8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32773134"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(model_path)[\"model\"])"
      ],
      "id": "32773134"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8eb8eeb"
      },
      "outputs": [],
      "source": [
        "all_df_test = pd.read_csv(\"/kaggle/input/hubmap-organ-segmentation/test.csv\")"
      ],
      "id": "a8eb8eeb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec7e35d3"
      },
      "outputs": [],
      "source": [
        "all_df_test.head(5)"
      ],
      "id": "ec7e35d3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d474e3d4"
      },
      "outputs": [],
      "source": [
        "id_l = all_df_test[\"id\"].values\n",
        "image_width_l = all_df_test[\"img_width\"].values"
      ],
      "id": "d474e3d4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a48f54f4"
      },
      "outputs": [],
      "source": [
        "id_l"
      ],
      "id": "a48f54f4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78fe667c"
      },
      "outputs": [],
      "source": [
        "#https://www.kaggle.com/bguberfain/memory-aware-rle-encoding\n",
        "#with transposed mask\n",
        "def rle_encode_less_memory(img):\n",
        "    #the image should be transposed\n",
        "    pixels = img.T.flatten()\n",
        "\n",
        "    # This simplified method requires first and last pixel to be zero\n",
        "    pixels[0] = 0\n",
        "    pixels[-1] = 0\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
        "    runs[1::2] -= runs[::2]\n",
        "\n",
        "    return ' '.join(str(x) for x in runs)"
      ],
      "id": "78fe667c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20adde9e"
      },
      "outputs": [],
      "source": [
        "names = []\n",
        "preds = []\n",
        "threshold = 0.5\n",
        "for idx in tqdm(range(len(id_l))):\n",
        "    id = id_l[idx]\n",
        "    img_w = image_width_l[idx]\n",
        "\n",
        "    image_path = \"/kaggle/input/hubmap-organ-segmentation/test_images/\" + str(id) + \".tiff\"\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, (512, 512))\n",
        "    img = img/255.0\n",
        "    img = torch.from_numpy(np.array(img))\n",
        "    img = img.permute(2,0,1)\n",
        "    img = img.unsqueeze(0)\n",
        "    img = img.to(device).float()\n",
        "    pred_mask = model(img)\n",
        "    pred_mask = pred_mask.squeeze(1)\n",
        "    pred_mask = pred_mask.cpu().data.numpy()\n",
        "    pred_mask = cv2.resize(pred_mask, (img_w, img_w))\n",
        "    pred_mask = pred_mask > threshold\n",
        "    rle = rle_encode_less_memory(pred_mask)\n",
        "    names.append(id)\n",
        "    preds.append(rle)\n"
      ],
      "id": "20adde9e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "291d5c22"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'id':names,'rle':preds})\n",
        "df.to_csv('submission.csv',index=False)"
      ],
      "id": "291d5c22"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "088c2c3b"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ],
      "id": "088c2c3b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15dc4e44"
      },
      "outputs": [],
      "source": [],
      "id": "15dc4e44"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 94.35558,
      "end_time": "2022-09-02T18:50:08.170125",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-09-02T18:48:33.814545",
      "version": "2.3.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}